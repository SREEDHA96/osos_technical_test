{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "a1KbALWc1-yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v7N2qQ3mbSe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary Library"
      ],
      "metadata": {
        "id": "5zq1vMa22HaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx pymupdf pandas pdfplumber langchain tiktoken openai faiss-cpu nomic optimum auto-gptq numpy transformers huggingface_hub langdetect rouge-score openpyxl"
      ],
      "metadata": {
        "id": "iTWHFeXO2ske"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nomic and hugging face login credential"
      ],
      "metadata": {
        "id": "_lbLhGTn2MIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nomic login # provide your nomic login token here\n",
        "from huggingface_hub import login\n",
        "login(token=\"\") #  ---> provide your hugging face login token here"
      ],
      "metadata": {
        "id": "_kI81brv4JPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries"
      ],
      "metadata": {
        "id": "wR-i-dHT2Rnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import csv\n",
        "import re\n",
        "import docx\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "import tiktoken\n",
        "import faiss\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nomic import embed\n",
        "from langdetect import detect\n",
        "from rouge_score import rouge_scorer\n",
        "from docx import Document\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline,\n",
        "    LlamaTokenizer,\n",
        "    LlamaForCausalLM\n",
        ")\n"
      ],
      "metadata": {
        "id": "PhXhSveY4APZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1\n",
        "\n",
        "# Reading the Publications:\n",
        "# • You have different types of publications: Ms Word files (.docx), PDF files (.pdf), and 'csv', 'xlsx', 'xls', 'xlsm' files.\n",
        "# • Write a Python function to extract the text from these files.\n",
        "# • Remember, the files might contain tables. Extract the text from these tables in a readable format, but do not\n",
        "# attempt to reconstruct their visual layout."
      ],
      "metadata": {
        "id": "ukW2YC440vgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6G0QXYbvANe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize tokenizer for token counting and chunking\n",
        "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def tokenize_and_chunk(text_with_page, global_chunk_counter):\n",
        "    \"\"\"\n",
        "    Tokenizes text into chunks (around 1000 tokens each) and adds metadata.\n",
        "\n",
        "    Args:\n",
        "        text_with_page (dict): Contains 'content', 'page_number', and optionally 'source'.\n",
        "        global_chunk_counter (int): Global chunk count to track sequence.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (List of chunked dicts, updated global chunk count)\n",
        "    \"\"\"\n",
        "    text = text_with_page['content']\n",
        "    page_number = text_with_page['page_number']\n",
        "    tokens = encoder.encode(text)\n",
        "\n",
        "    # Estimate average token length per word\n",
        "    avg_token_length = len(tokens) / len(text.split()) if len(text.split()) > 0 else 1\n",
        "    desired_chunk_size_in_tokens = 1000\n",
        "    tokens_per_chunk = int(desired_chunk_size_in_tokens * avg_token_length)\n",
        "\n",
        "    # Split into token chunks\n",
        "    chunks = [tokens[i: i + tokens_per_chunk] for i in range(0, len(tokens), tokens_per_chunk)]\n",
        "    chunked_data = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunked_data.append({\n",
        "            \"content\": encoder.decode(chunk),  # Decode tokens back to text\n",
        "            \"page_number\": page_number,\n",
        "            \"chunk_number\": global_chunk_counter,\n",
        "            \"source\": text_with_page.get('source', 'Unknown'),\n",
        "        })\n",
        "        global_chunk_counter += 1\n",
        "\n",
        "    return chunked_data, global_chunk_counter\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Dispatches file to appropriate extractor based on its extension.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file.\n",
        "\n",
        "    Returns:\n",
        "        list: List of dicts with extracted content and metadata.\n",
        "    \"\"\"\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.docx':\n",
        "        return extract_text_from_docx(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_text_and_tables_from_pdf(file_path)\n",
        "    elif file_extension in ['.csv', '.xlsx', '.xls', '.xlsm']:\n",
        "        return extract_all_tables(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    \"\"\"\n",
        "    Extracts paragraphs and tables from a Word (.docx) file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the DOCX file.\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted elements with page number and type (text/table).\n",
        "    \"\"\"\n",
        "    doc = docx.Document(file_path)\n",
        "    text = []\n",
        "    namespaces = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    page_number = 1\n",
        "\n",
        "    for element in doc.element.body:\n",
        "        if element.tag.endswith('p'):  # Paragraph\n",
        "            para_text = ' '.join([\n",
        "                run.text.strip() for run in element.findall('.//w:t', namespaces) if run.text.strip() != ''\n",
        "            ])\n",
        "            if para_text:\n",
        "                text.append({\n",
        "                    \"type\": \"text\",\n",
        "                    \"source\": file_path,\n",
        "                    \"page_number\": page_number,\n",
        "                    \"content\": para_text.strip(),\n",
        "                    \"table\": None\n",
        "                })\n",
        "\n",
        "        elif element.tag.endswith('tbl'):  # Table\n",
        "            table_data = []\n",
        "            for row in element.findall('.//w:tr', namespaces):\n",
        "                row_text = [cell.text.strip() for cell in row.findall('.//w:t', namespaces) if cell.text.strip()]\n",
        "                if row_text:\n",
        "                    table_data.append(\" | \".join(row_text))\n",
        "            if table_data:\n",
        "                text.append({\n",
        "                    \"type\": \"table\",\n",
        "                    \"source\": file_path,\n",
        "                    \"page_number\": page_number,\n",
        "                    \"content\": \"\\n\".join(table_data),\n",
        "                    \"table\": table_data\n",
        "                })\n",
        "\n",
        "        if len(text) % 5 == 0:  # Approximate page number increment\n",
        "            page_number += 1\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_text_and_tables_from_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text and tables from a PDF using both PyMuPDF and pdfplumber.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted content with page number and content type.\n",
        "    \"\"\"\n",
        "    combined_content = []\n",
        "    doc = fitz.open(file_path)\n",
        "\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page_num in range(len(doc)):\n",
        "            # Extract text using PyMuPDF\n",
        "            page = doc.load_page(page_num)\n",
        "            page_text = page.get_text(\"text\")\n",
        "            if page_text.strip():\n",
        "                combined_content.append({\n",
        "                    \"type\": \"text\",\n",
        "                    \"source\": file_path,\n",
        "                    \"page_number\": page_num + 1,\n",
        "                    \"content\": page_text.strip()\n",
        "                })\n",
        "\n",
        "            # Extract tables using pdfplumber\n",
        "            pdf_page = pdf.pages[page_num]\n",
        "            tables = pdf_page.extract_tables()\n",
        "\n",
        "            for table in tables:\n",
        "                formatted_table = []\n",
        "                for row in table:\n",
        "                    formatted_row = \"|\".join([str(cell) for cell in row if cell])\n",
        "                    formatted_table.append(formatted_row)\n",
        "                if formatted_table:\n",
        "                    combined_content.append({\n",
        "                        \"type\": \"table\",\n",
        "                        \"source\": file_path,\n",
        "                        \"page_number\": page_num + 1,\n",
        "                        \"content\": \"\\n\".join(formatted_table)\n",
        "                    })\n",
        "\n",
        "    return combined_content\n",
        "\n",
        "def extract_all_tables(file_path):\n",
        "    \"\"\"\n",
        "    Extracts tables from the first sheet of an Excel file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to Excel file.\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted table content.\n",
        "    \"\"\"\n",
        "    xl = pd.ExcelFile(file_path)\n",
        "    sheet_name = xl.sheet_names[0]\n",
        "    sheet_data = xl.parse(sheet_name)\n",
        "    return extract_tables_from_sheet(sheet_data, sheet_name, file_path)\n",
        "\n",
        "def extract_tables_from_sheet(sheet_data, sheet_name, file_path):\n",
        "    \"\"\"\n",
        "    Extracts structured tables from an Excel sheet.\n",
        "\n",
        "    Args:\n",
        "        sheet_data (DataFrame): Parsed Excel sheet.\n",
        "        sheet_name (str): Name of the sheet.\n",
        "        file_path (str): File path for reference.\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted tables with headers and data.\n",
        "    \"\"\"\n",
        "    tables = []\n",
        "    rows = sheet_data.values.tolist()\n",
        "    current_table = []\n",
        "    table_title = None\n",
        "\n",
        "    for row in rows:\n",
        "        if any(cell for cell in row):  # Non-empty row\n",
        "            if table_title is None:\n",
        "                table_title = row\n",
        "            else:\n",
        "                current_table.append(row)\n",
        "        else:\n",
        "            if current_table:\n",
        "                # Store the complete table\n",
        "                formatted_table = {\n",
        "                    \"type\": \"table\",\n",
        "                    \"source\": file_path,\n",
        "                    \"page_number\": sheet_name,\n",
        "                    \"title\": \" | \".join([str(cell) for cell in table_title]) if table_title else \"No Title\",\n",
        "                    \"table_data\": \"\\n\".join([\" | \".join([str(cell) for cell in row]) for row in current_table]),\n",
        "                    \"content\": \" | \".join([str(cell) for cell in table_title]) + \" | \" + \"\\n\".join([\" | \".join([str(cell) for cell in row]) for row in current_table])\n",
        "                }\n",
        "                tables.append(formatted_table)\n",
        "                table_title = None\n",
        "                current_table = []\n",
        "\n",
        "    # Capture last table if file ends without an empty row\n",
        "    if current_table:\n",
        "        formatted_table = {\n",
        "            \"type\": \"table\",\n",
        "            \"source\": file_path,\n",
        "            \"page_number\": sheet_name,\n",
        "            \"title\": \" | \".join([str(cell) for cell in table_title]) if table_title else \"No Title\",\n",
        "            \"table_data\": \"\\n\".join([\" | \".join([str(cell) for cell in row]) for row in current_table]),\n",
        "            \"content\": \" | \".join([str(cell) for cell in table_title]) + \" | \" + \"\\n\".join([\" | \".join([str(cell) for cell in row]) for row in current_table])\n",
        "        }\n",
        "        tables.append(formatted_table)\n",
        "\n",
        "    return tables\n",
        "\n",
        "def process_and_save_chunks(file_path):\n",
        "    \"\"\"\n",
        "    Master function to extract, tokenize, chunk, and save structured text.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): File to be processed.\n",
        "\n",
        "    Outputs:\n",
        "        - Console performance report.\n",
        "        - CSV file with chunked content.\n",
        "    \"\"\"\n",
        "    extracted_text = extract_text_from_file(file_path)\n",
        "    print(f\"Extracted text from {file_path}:\")\n",
        "\n",
        "    all_chunks = []\n",
        "    global_chunk_counter = 1\n",
        "    total_tokens = 0\n",
        "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    start_time = time.time()\n",
        "\n",
        "    for item in extracted_text:\n",
        "        chunks, global_chunk_counter = tokenize_and_chunk(item, global_chunk_counter)\n",
        "        for chunk in chunks:\n",
        "            tokens_in_chunk = len(encoder.encode(chunk[\"content\"]))\n",
        "            total_tokens += tokens_in_chunk\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    tps = total_tokens / total_time if total_time > 0 else 0\n",
        "\n",
        "    # Print performance stats\n",
        "    print(f\"\\n--- Performance Stats ---\")\n",
        "    print(f\"Total tokens processed: {total_tokens}\")\n",
        "    print(f\"Total time: {total_time:.2f} seconds\")\n",
        "    print(f\"Tokens per second (TPS): {tps:.2f}\")\n",
        "\n",
        "    # Save chunks to CSV\n",
        "    csv_file = file_name + \"_chunked_text_details.csv\"\n",
        "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"source\", \"page_number\", \"chunk_number\", \"content\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_chunks)\n",
        "\n",
        "    print(f\"Chunk details saved to {csv_file}\")\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/osos_technical_testing_folder/Dr.X Files/new-approaches-and-procedures-for-cancer-treatment.pdf\"\n",
        "process_and_save_chunks(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Task 2\n",
        "\n",
        "#   Breaking Down the Publications:\n",
        "# • The publications are extensive, so you need to divide them into smaller, manageable parts.\n",
        "# • Use the cl100k_base tokenizer to break the text into chunks.\n",
        "# • For each chunk, record:\n",
        "# o The file name (source).\n",
        "# o The page number.\n",
        "# o The chunk number (hint: add a counter).\n",
        "# o The text of that chunk."
      ],
      "metadata": {
        "id": "DU92BbEa06_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGHwywIASXKz"
      },
      "outputs": [],
      "source": [
        "# Initialize the tokenizer used by OpenAI models (cl100k_base)\n",
        "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# Load the CSV file that contains chunked text, please provide the file path here\n",
        "csv_file = '/content/drive/MyDrive/osos_technical/csv_ouput_osos/new-approaches-and-procedures-for-cancer-treatment_chunked_text_details (1).csv'\n",
        "csv_file_name = os.path.basename(csv_file)\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Initialize lists and counters\n",
        "embedding_results = []\n",
        "total_tokens = 0  # Track total tokens processed\n",
        "\n",
        "start_time = time.time()  # Track start time for performance stats\n",
        "\n",
        "# Process each chunk from the CSV\n",
        "for index, row in df.iterrows():\n",
        "    text = row['content']\n",
        "\n",
        "    # Count tokens using the tokenizer\n",
        "    token_count = len(encoder.encode(text))\n",
        "    total_tokens += token_count\n",
        "\n",
        "    try:\n",
        "        # Generate embedding using the embedding model\n",
        "        output = embed.text(\n",
        "            texts=[text],\n",
        "            model='nomic-embed-text-v1',\n",
        "        )\n",
        "        # Extract embedding vector\n",
        "        embedding = np.array(output['embeddings'])[0]\n",
        "\n",
        "        # Append results to the list\n",
        "        embedding_results.append({\n",
        "            'chunk_number': row['chunk_number'],\n",
        "            'page_number': row['page_number'],\n",
        "            'content': text,\n",
        "            'embedding': embedding.tolist(),\n",
        "            'token_count': token_count\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error embedding chunk {row['chunk_number']}: {e}\")\n",
        "\n",
        "\n",
        "# Performance Tracking\n",
        "\n",
        "# Calculate total elapsed time\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Compute tokens processed per second\n",
        "tps = total_tokens / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "# Print performance summary\n",
        "print(f\"\\n--- Embedding Performance Stats ---\")\n",
        "print(f\"Total Chunks: {len(embedding_results)}\")\n",
        "print(f\"Total Tokens: {total_tokens}\")\n",
        "print(f\"Total Time: {elapsed_time:.2f} seconds\")\n",
        "print(f\"Tokens per Second (TPS): {tps:.2f}\")\n",
        "\n",
        "\n",
        "# Save the embedding results to a new CSV\n",
        "embedding_df = pd.DataFrame(embedding_results)\n",
        "\n",
        "\n",
        "# give your required file path here:\n",
        "file_path = '/content/'\n",
        "output_path = file_path + csv_file_name.replace('.csv', '_embedding_results.csv')\n",
        "embedding_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Embeddings saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "\n",
        "\n",
        "# Building a Vector Database:\n",
        "# • Create a vector database from the chunked publications.\n",
        "# • Use nomic embedding model to generate vector embeddings for each chunk.\n",
        "# • Store the embeddings along with the chunk metadata in a vector database.\n",
        "\n",
        "# Task 4\n",
        "# Creating a RAG Q&A System:\n",
        "# • Develop a RAG-based Q&A system that can answer questions about the publications.\n",
        "# • When a user asks a question, the system should:\n",
        "# o Generate a vector embedding for the question.\n",
        "# o Retrieve the most relevant chunks from the vector database.\n",
        "# o Use llama LLM to generate an answer based on the retrieved chunks.\n",
        "# • Hint: Can your code answer to user’s questions based of the previous question?"
      ],
      "metadata": {
        "id": "t77xTDUA1D-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model (GPTQ)\n",
        "model_id = \"TheBloke/Llama-2-7B-Chat-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    revision=\"main\"\n",
        ")\n",
        "\n",
        "# LLaMA pipeline\n",
        "llama_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\",\n",
        "    return_full_text=False  # This ensures only the answer is returned\n",
        ")\n",
        "\n",
        "# Load embeddings\n",
        "csv_file = \"/content/new-approaches-and-procedures-for-cancer-treatment_chunked_text_details (1)_embedding_results.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Convert string embeddings to numpy array\n",
        "embeddings = np.array(df['embedding'].apply(eval).tolist())\n",
        "\n",
        "# Create FAISS index\n",
        "embedding_dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "index.add(embeddings)\n",
        "\n",
        "# Metadata\n",
        "contents = df['content'].tolist()\n",
        "chunk_numbers = df['chunk_number'].tolist()\n",
        "page_numbers = df['page_number'].tolist()\n",
        "\n",
        "# Tokenizer for performance stats\n",
        "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# RAG function with proper prompt handling\n",
        "def rag_query_with_history(query, conversation_history, top_k=3, max_new_tokens=200):\n",
        "    \"\"\"\n",
        "    This function performs a Retrieval-Augmented Generation (RAG) query by retrieving relevant chunks from the FAISS index,\n",
        "    constructing a prompt with the retrieved context and conversation history, and generating a response using the LLaMA model.\n",
        "\n",
        "    Parameters:\n",
        "    - query (str): The user's query to be answered.\n",
        "    - conversation_history (list): A list of past Q&A pairs to provide context for the current query.\n",
        "    - top_k (int): The number of top retrieved chunks to use for context (default is 3).\n",
        "    - max_new_tokens (int): The maximum number of tokens to generate for the answer (default is 200).\n",
        "\n",
        "    Returns:\n",
        "    - response_text (str): The generated response to the query.\n",
        "    - conversation_history (list): The updated conversation history with the current Q&A added.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Embed the query\n",
        "        query_embedding = embed.text(texts=[query], model=\"nomic-embed-text-v1\")[\"embeddings\"][0]\n",
        "        query_vector = np.array(query_embedding).reshape(1, -1)\n",
        "\n",
        "        # Search FAISS index\n",
        "        distances, indices = index.search(query_vector, top_k)\n",
        "        retrieved_chunks = [contents[idx] for idx in indices[0]]\n",
        "\n",
        "        # Deduplicate and filter\n",
        "        seen = set()\n",
        "        cleaned_chunks = []\n",
        "        for chunk in retrieved_chunks:\n",
        "            chunk = chunk.strip()\n",
        "            if chunk not in seen and len(chunk) > 30:\n",
        "                seen.add(chunk)\n",
        "                cleaned_chunks.append(chunk)\n",
        "\n",
        "        # Prepare context\n",
        "        context = \"\\n\\n\".join(f\"- {chunk}\" for chunk in cleaned_chunks)\n",
        "\n",
        "        # Clean conversation history (optional)\n",
        "        trimmed_history = \"\\n\".join(conversation_history[-4:])  # Only last two QA pairs\n",
        "\n",
        "        # Prompt construction\n",
        "        prompt = f\"\"\"You are a helpful research assistant. Use the context below to answer the question clearly and concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Conversation history:\n",
        "{trimmed_history}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Measure TPS\n",
        "        prompt_tokens = len(encoder.encode(prompt))\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Generate answer (temperature adjusted, deterministic)\n",
        "        result = llama_pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False, temperature=0.7)[0][\"generated_text\"]\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Extract response\n",
        "        response_text = result.strip()\n",
        "\n",
        "        # Token count\n",
        "        response_tokens = len(encoder.encode(response_text))\n",
        "        total_tokens = prompt_tokens + response_tokens\n",
        "        elapsed = end_time - start_time\n",
        "        tps = total_tokens / elapsed if elapsed > 0 else 0\n",
        "\n",
        "        # Logging\n",
        "        print(f\"\\nPerformance:\")\n",
        "        print(f\" - Prompt tokens: {prompt_tokens}\")\n",
        "        print(f\" - Response tokens: {response_tokens}\")\n",
        "        print(f\" - Total tokens: {total_tokens}\")\n",
        "        print(f\" - Time: {elapsed:.2f} sec\")\n",
        "        print(f\" - Tokens/sec: {tps:.2f}\")\n",
        "\n",
        "        # Update history\n",
        "        conversation_history.append(f\"Question: {query}\")\n",
        "        conversation_history.append(f\"Answer: {response_text}\")\n",
        "\n",
        "        return response_text, conversation_history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing query: {e}\")\n",
        "        return \"Sorry, something went wrong with this query.\", conversation_history\n",
        "\n",
        "\n",
        "# Initialize history\n",
        "conversation_history = []\n",
        "\n",
        "# Queries\n",
        "queries = [\n",
        "    \"What are the main advantages of targeted therapy in cancer treatment?\",\n",
        "    \"What are the potential disadvantages associated with gene therapy, particularly in relation to RNA interference (RNAi)?\",\n",
        "    \"How do natural antioxidants contribute to cancer prevention or treatment, and what are the common compounds under clinical trials?\"\n",
        "]\n",
        "\n",
        "# Run all queries\n",
        "for i, query in enumerate(queries):\n",
        "    print(f\"\\nQuery {i+1}: {query}\")\n",
        "    answer, conversation_history = rag_query_with_history(query, conversation_history)\n",
        "    print(f\"\\nAnswer {i+1}: {answer}\")\n"
      ],
      "metadata": {
        "id": "xtwAHow_wlVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translation task"
      ],
      "metadata": {
        "id": "2KQyS-UxIp32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5\n",
        "# Translating the publications:\n",
        "# • Dr. X wrote some publications in different languages.\n",
        "# • Build a tool (using any LLM) to translate between any language to English or Arabic.\n",
        "# • A plus: Strive to maintain the original structure and formatting of the publications after translation.\n",
        "# • Find creative ways to improve the translation accuracy and fluency."
      ],
      "metadata": {
        "id": "2JdtqxBx1TVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translation task -- Example: French to English, Spanish to English from txt/pdf files."
      ],
      "metadata": {
        "id": "IL3Bt9Lz260C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Force CPU usage for all models\n",
        "# ----------------------------\n",
        "device = -1  # CPU\n",
        "print(\"[*] Loading translation models (Helsinki, CPU)...\")\n",
        "\n",
        "translator_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\", device=device)\n",
        "translator_ar = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ar\", device=device)\n",
        "\n",
        "# ----------------------------\n",
        "# Load Grammar Correction Model\n",
        "# ----------------------------\n",
        "print(\"[*] Loading grammar correction model (T5-small, CPU)...\")\n",
        "fluency_model_id = \"vennify/t5-base-grammar-correction\"\n",
        "fluency_tokenizer = AutoTokenizer.from_pretrained(fluency_model_id)\n",
        "fluency_model = AutoModelForSeq2SeqLM.from_pretrained(fluency_model_id)\n",
        "\n",
        "def improve_fluency(text):\n",
        "    \"\"\"\n",
        "    Improve the fluency of English text using a grammar correction model.\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw English text.\n",
        "\n",
        "    Returns:\n",
        "        str: Grammatically improved text.\n",
        "    \"\"\"\n",
        "    print(\"[*] Polishing translation (fluency)...\")\n",
        "    input_text = \"grammar: \" + text.strip().replace(\"\\n\", \" \")\n",
        "    inputs = fluency_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = fluency_model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n",
        "    corrected = fluency_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return corrected\n",
        "\n",
        "# ----------------------------\n",
        "# Language Mapping\n",
        "# ----------------------------\n",
        "LANG_MAP = {\n",
        "    \"en\": \"English\",\n",
        "    \"es\": \"Spanish\",\n",
        "    \"fr\": \"French\",\n",
        "    \"de\": \"German\",\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"ru\": \"Russian\",\n",
        "    \"zh-cn\": \"Chinese\",\n",
        "    \"tr\": \"Turkish\"\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# File Extraction\n",
        "# ----------------------------\n",
        "def extract_text(file_path):\n",
        "    \"\"\"\n",
        "    Extract text from a file based on its extension.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input file (.pdf, .docx, .txt).\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted plain text.\n",
        "    \"\"\"\n",
        "    ext = file_path.lower().split('.')[-1]\n",
        "    if ext == \"pdf\":\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif ext == \"docx\":\n",
        "        return extract_from_docx(file_path)\n",
        "    elif ext == \"txt\":\n",
        "        return extract_from_txt(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "def extract_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file using pdfplumber.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: Text extracted from the PDF.\n",
        "    \"\"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        return \"\\n\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
        "\n",
        "def extract_from_docx(docx_path):\n",
        "    \"\"\"\n",
        "    Extract text from a DOCX file using python-docx.\n",
        "\n",
        "    Args:\n",
        "        docx_path (str): Path to the DOCX file.\n",
        "\n",
        "    Returns:\n",
        "        str: Text extracted from the DOCX.\n",
        "    \"\"\"\n",
        "    doc = Document(docx_path)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "def extract_from_txt(txt_path):\n",
        "    \"\"\"\n",
        "    Extract text from a plain text file.\n",
        "\n",
        "    Args:\n",
        "        txt_path (str): Path to the TXT file.\n",
        "\n",
        "    Returns:\n",
        "        str: Contents of the TXT file.\n",
        "    \"\"\"\n",
        "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "# ----------------------------\n",
        "# Split for Max Length Translation\n",
        "# ----------------------------\n",
        "def split_text(text, max_len=512):\n",
        "    \"\"\"\n",
        "    Split text into smaller chunks for translation based on sentence boundaries.\n",
        "\n",
        "    Args:\n",
        "        text (str): Full input text.\n",
        "        max_len (int): Max character length for each chunk.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of text chunks.\n",
        "    \"\"\"\n",
        "    sentences = text.split('. ')\n",
        "    chunks, current = [], ''\n",
        "    for sent in sentences:\n",
        "        if len(current) + len(sent) < max_len:\n",
        "            current += sent + '. '\n",
        "        else:\n",
        "            chunks.append(current.strip())\n",
        "            current = sent + '. '\n",
        "    if current:\n",
        "        chunks.append(current.strip())\n",
        "    return chunks\n",
        "\n",
        "# ----------------------------\n",
        "# Translate Logic with TPS\n",
        "# ----------------------------\n",
        "def translate_text(text, target_lang=\"en\"):\n",
        "    \"\"\"\n",
        "    Translate input text into the desired target language (English or Arabic).\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text in any language.\n",
        "        target_lang (str): 'en' for English or 'ar' for Arabic.\n",
        "\n",
        "    Returns:\n",
        "        str: Translated text.\n",
        "    \"\"\"\n",
        "    print(f\"[*] Translating to {target_lang.upper()}...\")\n",
        "\n",
        "    detected_lang = detect(text)\n",
        "    print(f\"[*] Detected source language: {LANG_MAP.get(detected_lang, 'Unknown')}\")\n",
        "\n",
        "    chunks = split_text(text)\n",
        "    total_tokens = 0\n",
        "    start_time = time.time()\n",
        "    translated_chunks = []\n",
        "\n",
        "    if target_lang == \"ar\":\n",
        "        # Step 1: Translate to English if source is not English\n",
        "        if detected_lang != \"en\":\n",
        "            print(\"[*] Intermediate translation: Source → English\")\n",
        "            temp_chunks = []\n",
        "            for chunk in chunks:\n",
        "                tokenized_input = translator_en.tokenizer(chunk, return_tensors=\"pt\")\n",
        "                num_tokens = len(tokenized_input[\"input_ids\"][0])\n",
        "                total_tokens += num_tokens\n",
        "                translated = translator_en(chunk, max_length=512)\n",
        "                temp_chunks.append(translated[0][\"translation_text\"])\n",
        "        else:\n",
        "            temp_chunks = chunks\n",
        "\n",
        "        # Step 2: Translate English → Arabic\n",
        "        print(\"[*] Final translation: English → Arabic\")\n",
        "        for chunk in temp_chunks:\n",
        "            tokenized_input = translator_ar.tokenizer(chunk, return_tensors=\"pt\")\n",
        "            num_tokens = len(tokenized_input[\"input_ids\"][0])\n",
        "            total_tokens += num_tokens\n",
        "            translated = translator_ar(chunk, max_length=512)\n",
        "            translated_chunks.append(translated[0][\"translation_text\"])\n",
        "\n",
        "    elif target_lang == \"en\":\n",
        "        # Translate directly to English\n",
        "        for chunk in chunks:\n",
        "            tokenized_input = translator_en.tokenizer(chunk, return_tensors=\"pt\")\n",
        "            num_tokens = len(tokenized_input[\"input_ids\"][0])\n",
        "            total_tokens += num_tokens\n",
        "            translated = translator_en(chunk, max_length=512)\n",
        "            translated_chunks.append(translated[0][\"translation_text\"])\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported target language: {target_lang}\")\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    tps = total_tokens / elapsed_time if elapsed_time > 0 else 0\n",
        "    print(f\"[*] Processed {total_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
        "    print(f\"[*] Tokens per second: {tps:.2f}\")\n",
        "\n",
        "    return \"\\n\\n\".join(translated_chunks)\n",
        "\n",
        "# ----------------------------\n",
        "# Save DOCX Output\n",
        "# ----------------------------\n",
        "def save_as_docx(text, output_path):\n",
        "    \"\"\"\n",
        "    Save translated text as a DOCX file.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be saved.\n",
        "        output_path (str): Destination file path (.docx).\n",
        "    \"\"\"\n",
        "    doc = Document()\n",
        "    for para in text.split(\"\\n\\n\"):\n",
        "        doc.add_paragraph(para.strip())\n",
        "    doc.save(output_path)\n",
        "\n",
        "# ----------------------------\n",
        "# Main Processor\n",
        "# ----------------------------\n",
        "def process_document(file_path, target_lang=\"en\"):\n",
        "    \"\"\"\n",
        "    Main document processing pipeline: extract, translate, polish (if English), save.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input file.\n",
        "        target_lang (str): Desired translation target ('en' or 'ar').\n",
        "    \"\"\"\n",
        "    print(f\"[+] Processing file: {file_path}\")\n",
        "    text = extract_text(file_path)\n",
        "    detected = detect(text)\n",
        "    print(f\"[*] Detected language: {LANG_MAP.get(detected, 'Unknown')}\")\n",
        "\n",
        "    translated = translate_text(text, target_lang=target_lang)\n",
        "\n",
        "    # Apply grammar polishing only if translating to English\n",
        "    if target_lang == \"en\":\n",
        "        polished = improve_fluency(translated)\n",
        "    else:\n",
        "        polished = translated\n",
        "\n",
        "    # Save output\n",
        "    lang_suffix = \"_translated_en\" if target_lang == \"en\" else \"_translated_ar\"\n",
        "    output_path = os.path.splitext(file_path)[0] + lang_suffix + \".docx\"\n",
        "    save_as_docx(polished, output_path)\n",
        "    print(f\"Saved translated file: {output_path}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Example Usage\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/drive/MyDrive/osos_technical_testing_folder/transalation_files/osos_spanish_file.txt\"\n",
        "    process_document(file_path, target_lang=\"ar\")  # Change 'ar' to 'en' for English output"
      ],
      "metadata": {
        "id": "eFfHlRn-wn9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6\n",
        "# Finding the Main Ideas:\n",
        "# • Create a tool (using any LLM) to summarize the publications.\n",
        "# • Evaluate the quality of your summaries using the ROUGE metric.\n",
        "# • Experiment with different summarization techniques and prompt strategies and record the result."
      ],
      "metadata": {
        "id": "hL3107V71iUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### summarization of the given text"
      ],
      "metadata": {
        "id": "xzBWYn8QfOmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a summarization model (e.g., BART or T5)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def summarize_text(text, max_length=150):\n",
        "    \"\"\"\n",
        "    Summarizes a given input text using a pretrained summarization model.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The full input text to summarize.\n",
        "    - max_length (int): The maximum number of tokens in the summary. Default is 150.\n",
        "\n",
        "    Returns:\n",
        "    - summarized_text (str): The summary generated by the model.\n",
        "\n",
        "    The function also prints performance metrics such as:\n",
        "    - Number of input tokens\n",
        "    - Time taken to summarize\n",
        "    - Tokens processed per second (TPS)\n",
        "    \"\"\"\n",
        "    start_time = time.time()  # Start the timer\n",
        "\n",
        "    # Tokenize the input text to calculate tokens\n",
        "    tokenized_input = summarizer.tokenizer(text, return_tensors=\"pt\")\n",
        "    num_tokens = len(tokenized_input[\"input_ids\"][0])  # Number of tokens in the input text\n",
        "\n",
        "    # Summarize the text\n",
        "    summary = summarizer(text, max_length=max_length, min_length=50, do_sample=False)\n",
        "    summarized_text = summary[0]['summary_text']\n",
        "\n",
        "    # Calculate time taken for summarization\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Calculate tokens per second (TPS)\n",
        "    tps = num_tokens / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "    print(f\"[*] Processed {num_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
        "    print(f\"[*] Tokens per second: {tps:.2f}\")\n",
        "\n",
        "    return summarized_text\n",
        "\n",
        "# Example publication text (could be read from a file)\n",
        "publication_text = \"\"\"\n",
        "This is a long publication about advancements in AI. Artificial intelligence (AI) is the simulation of human intelligence in machines. The goal is to create systems capable of performing tasks that typically require human intelligence. These tasks include speech recognition, decision-making, and visual perception. In recent years, AI has seen significant growth in applications ranging from self-driving cars to healthcare diagnostics. With advancements in deep learning and neural networks, AI is expected to revolutionize industries worldwide. However, challenges like ethical considerations and data privacy remain.\n",
        "\"\"\"\n",
        "\n",
        "# Summarize the publication\n",
        "summary = summarize_text(publication_text)\n",
        "print(\"\\nSummary:\\n\", summary)\n"
      ],
      "metadata": {
        "id": "UZSmgvIvfOZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with Different Summarization Techniques & Prompt Strategies\n",
        "\n",
        "# 1) Try different summarization models (e.g., T5, PEGASUS).\n",
        "\n",
        "# 2) Try prompt variations (e.g., \"summarize:\", \"tl;dr:\", \"In summary, ...\").\n",
        "\n",
        "# 3) Log and compare output quality, token usage, and tokens/sec.\n",
        "\n"
      ],
      "metadata": {
        "id": "S9SRfnoe10_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Text to summarize\n",
        "text = \"\"\"\n",
        "This is a long publication about advancements in AI. Artificial intelligence (AI) is the simulation of human intelligence in machines. The goal is to create systems capable of performing tasks that typically require human intelligence. These tasks include speech recognition, decision-making, and visual perception. In recent years, AI has seen significant growth in applications ranging from self-driving cars to healthcare diagnostics. With advancements in deep learning and neural networks, AI is expected to revolutionize industries worldwide. However, challenges like ethical considerations and data privacy remain.\n",
        "\"\"\"\n",
        "\n",
        "# Models to compare\n",
        "summarization_models = {\n",
        "    \"BART\": \"facebook/bart-large-cnn\",\n",
        "    \"PEGASUS\": \"google/pegasus-xsum\",\n",
        "    \"T5\": \"t5-base\"\n",
        "}\n",
        "\n",
        "# Prompt strategies\n",
        "prompt_styles = {\n",
        "    \"default\": lambda t: t,\n",
        "    \"summarize_prefix\": lambda t: \"summarize: \" + t,\n",
        "    \"tl;dr\": lambda t: \"tl;dr: \" + t\n",
        "}\n",
        "\n",
        "# Run all combinations\n",
        "for model_name, model_id in summarization_models.items():\n",
        "    summarizer = pipeline(\"summarization\", model=model_id)\n",
        "\n",
        "    for prompt_name, prompt_func in prompt_styles.items():\n",
        "        prompt_text = prompt_func(text)\n",
        "\n",
        "        tokenized = summarizer.tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "        num_tokens = len(tokenized[\"input_ids\"][0])\n",
        "\n",
        "        start = time.time()\n",
        "        summary = summarizer(prompt_text, max_length=150, min_length=50, do_sample=False)\n",
        "        end = time.time()\n",
        "\n",
        "        elapsed = end - start\n",
        "        tps = num_tokens / elapsed if elapsed > 0 else 0\n",
        "        summarized_text = summary[0][\"summary_text\"]\n",
        "\n",
        "        print(f\"\\n=== {model_name} | Prompt: {prompt_name} ===\")\n",
        "        print(f\"Tokens: {num_tokens}, Time: {elapsed:.2f}s, TPS: {tps:.2f}\")\n",
        "        print(\"Summary:\", summarized_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZGdU0to18B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## rouge metrics evaulation"
      ],
      "metadata": {
        "id": "T6waBP4RpZel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_summary(reference_summary, generated_summary):\n",
        "    \"\"\"\n",
        "    Evaluates the quality of a generated summary by comparing it to a human-written reference summary\n",
        "    using ROUGE metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - reference_summary (str): The ground-truth or human-made summary of the original text.\n",
        "    - generated_summary (str): The summary generated by the model.\n",
        "\n",
        "    Returns:\n",
        "    - scores (dict): A dictionary containing the ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
        "\n",
        "    ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores measure:\n",
        "    - ROUGE-1: Overlap of unigrams (individual words).\n",
        "    - ROUGE-2: Overlap of bigrams (two-word sequences).\n",
        "    - ROUGE-L: Longest common subsequence-based similarity.\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference_summary, generated_summary)\n",
        "    return scores\n",
        "\n",
        "# Example reference summary (human-made)\n",
        "reference_summary = \"\"\"\n",
        "AI is the simulation of human intelligence in machines to perform tasks like speech recognition and decision-making. It has grown in applications like self-driving cars and healthcare diagnostics, but challenges such as ethics and privacy remain.\n",
        "\"\"\"\n",
        "\n",
        "# Evaluate the model's summary using ROUGE\n",
        "generated_summary = summarize_text(publication_text)\n",
        "rouge_scores = evaluate_summary(reference_summary, generated_summary)\n",
        "print(\"ROUGE Scores:\\n\", rouge_scores)\n"
      ],
      "metadata": {
        "id": "jDaE5_8Ppb1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7\n",
        "# Performance Measurement:\n",
        "# • During the embedding generation, translation, summarization, and RAG processes, record the \"tokens per\n",
        "# second\" processed by the LLM. This will help us understand the efficiency of your algorithms.\n",
        "\n",
        "# Answer\n",
        "\n",
        "# To provide a complete and detailed implementation for performance measurement across all steps (embedding generation, translation, summarization, and RAG processes), I've added tokens per second (TPS) tracking throughout each process."
      ],
      "metadata": {
        "id": "Bl7KtHpo1pT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8\n",
        "\n",
        "# Be Creative:\n",
        "# • Demonstrate your creativity. For example:\n",
        "# o Develop advanced chunking methods.\n",
        "# o Enhance the accuracy and clarity of your translations.\n",
        "# o Create unique evaluation metrics for the algorithms.\n",
        "# o Implement create algorithms for tables and charts text extraction."
      ],
      "metadata": {
        "id": "u4jvXyX512O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Develop advanced chunking methods.\n",
        "\n",
        "#  Smart Table-Aware Chunking\n",
        "# For table-heavy content:\n",
        "\n",
        "# Preserve full tables in one chunk regardless of token count.\n",
        "\n",
        "# Flag them as \"is_table\": True in the chunk metadata."
      ],
      "metadata": {
        "id": "PUJlu-ZV-01m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize tokenizer\n",
        "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
        "nltk.data.clear_cache()\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def tokenize_and_chunk_semantic(text_with_page, global_chunk_counter, max_tokens=1000):\n",
        "    \"\"\"\n",
        "    Tokenizes and chunks the provided text into semantic chunks based on sentence boundaries,\n",
        "    ensuring each chunk does not exceed the specified maximum token count.\n",
        "\n",
        "    Args:\n",
        "    - text_with_page (dict): A dictionary containing 'content' (text) and 'page_number'.\n",
        "    - global_chunk_counter (int): A counter to keep track of chunk numbers across pages.\n",
        "    - max_tokens (int, optional): The maximum number of tokens per chunk (default is 1000).\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of chunked text entries, each with 'content', 'page_number', and 'chunk_number'.\n",
        "    - int: The updated chunk counter.\n",
        "    \"\"\"\n",
        "    text = text_with_page['content']\n",
        "    page_number = text_with_page['page_number']\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    current_tokens = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_tokens = encoder.encode(sentence)\n",
        "        if current_tokens + len(sentence_tokens) <= max_tokens:\n",
        "            current_chunk += \" \" + sentence\n",
        "            current_tokens += len(sentence_tokens)\n",
        "        else:\n",
        "            # Save current chunk\n",
        "            chunks.append({\n",
        "                \"content\": current_chunk.strip(),\n",
        "                \"page_number\": page_number,\n",
        "                \"chunk_number\": global_chunk_counter,\n",
        "                \"source\": text_with_page.get('source', 'Unknown')\n",
        "            })\n",
        "            global_chunk_counter += 1\n",
        "            current_chunk = sentence\n",
        "            current_tokens = len(sentence_tokens)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append({\n",
        "            \"content\": current_chunk.strip(),\n",
        "            \"page_number\": page_number,\n",
        "            \"chunk_number\": global_chunk_counter,\n",
        "            \"source\": text_with_page.get('source', 'Unknown')\n",
        "        })\n",
        "        global_chunk_counter += 1\n",
        "\n",
        "    return chunks, global_chunk_counter\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text or tables from a document based on its file extension.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): The path to the file to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of extracted text or tables, each represented as a dictionary.\n",
        "    \"\"\"\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "    if file_extension == '.docx':\n",
        "        return extract_text_from_docx(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_text_and_tables_from_pdf(file_path)\n",
        "    elif file_extension in ['.csv', '.xlsx', '.xls', '.xlsm']:\n",
        "        return extract_all_tables(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text and tables from a DOCX file.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): The path to the DOCX file.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of dictionaries, each representing a paragraph or table.\n",
        "    \"\"\"\n",
        "    doc = docx.Document(file_path)\n",
        "    text = []\n",
        "    namespaces = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}\n",
        "    page_number = 1\n",
        "    for element in doc.element.body:\n",
        "        if element.tag.endswith('p'):\n",
        "            para_text = ' '.join([run.text.strip() for run in element.findall('.//w:t', namespaces) if run.text.strip() != ''])\n",
        "            if para_text:\n",
        "                text.append({\n",
        "                    \"type\": \"text\",\n",
        "                    \"source\": file_path,\n",
        "                    \"page_number\": page_number,\n",
        "                    \"content\": para_text.strip(),\n",
        "                    \"table\": None\n",
        "                })\n",
        "        elif element.tag.endswith('tbl'):\n",
        "            table_data = []\n",
        "            for row in element.findall('.//w:tr', namespaces):\n",
        "                row_text = [cell.text.strip() for cell in row.findall('.//w:t', namespaces) if cell.text.strip()]\n",
        "                if row_text:\n",
        "                    table_data.append(\" | \".join(row_text))\n",
        "            if table_data:\n",
        "                text.append({\n",
        "                    \"type\": \"table\",\n",
        "                    \"source\": file_path,\n",
        "                    \"page_number\": page_number,\n",
        "                    \"content\": \"\\n\".join(table_data),\n",
        "                    \"table\": table_data\n",
        "                })\n",
        "        if len(text) % 5 == 0:\n",
        "            page_number += 1\n",
        "    return text\n",
        "\n",
        "def extract_text_and_tables_from_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extracts text and tables from a PDF file.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): The path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of dictionaries, each representing a page's text or table data.\n",
        "    \"\"\"\n",
        "    combined_content = []\n",
        "    doc = fitz.open(file_path)\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            page_text = page.get_text(\"text\")\n",
        "            if page_text.strip():\n",
        "                combined_content.append({\n",
        "                    \"type\": \"text\",\n",
        "                    \"source\": file_path,\n",
        "                    \"page_number\": page_num + 1,\n",
        "                    \"content\": page_text.strip()\n",
        "                })\n",
        "            pdf_page = pdf.pages[page_num]\n",
        "            tables = pdf_page.extract_tables()\n",
        "            for table in tables:\n",
        "                formatted_table = []\n",
        "                for row in table:\n",
        "                    formatted_row = \"|\".join([str(cell) for cell in row if cell])\n",
        "                    formatted_table.append(formatted_row)\n",
        "                if formatted_table:\n",
        "                    combined_content.append({\n",
        "                        \"type\": \"table\",\n",
        "                        \"source\": file_path,\n",
        "                        \"page_number\": page_num + 1,\n",
        "                        \"content\": \"\\n\".join(formatted_table)\n",
        "                    })\n",
        "    return combined_content\n",
        "\n",
        "def extract_all_tables(file_path):\n",
        "    \"\"\"\n",
        "    Extracts tables from Excel files (CSV, XLSX, etc.).\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): The path to the Excel file.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of dictionaries, each representing a table.\n",
        "    \"\"\"\n",
        "    xl = pd.ExcelFile(file_path)\n",
        "    sheet_name = xl.sheet_names[0]\n",
        "    sheet_data = xl.parse(sheet_name)\n",
        "    return extract_tables_from_sheet(sheet_data, sheet_name, file_path)\n",
        "\n",
        "def extract_tables_from_sheet(sheet_data, sheet_name, file_path):\n",
        "    \"\"\"\n",
        "    Extracts tables from a specific Excel sheet.\n",
        "\n",
        "    Args:\n",
        "    - sheet_data (pd.DataFrame): The DataFrame containing sheet data.\n",
        "    - sheet_name (str): The name of the sheet.\n",
        "    - file_path (str): The path to the Excel file.\n",
        "\n",
        "    Returns:\n",
        "    - list: A list of table dictionaries.\n",
        "    \"\"\"\n",
        "    tables = []\n",
        "    rows = sheet_data.values.tolist()\n",
        "    current_table = []\n",
        "    table_title = None\n",
        "    for row in rows:\n",
        "        if any(cell for cell in row):\n",
        "            if table_title is None:\n",
        "                table_title = row\n",
        "            else:\n",
        "                current_table.append(row)\n",
        "        else:\n",
        "            if current_table:\n",
        "                formatted_table = {\n",
        "                    \"type\": \"table\",\n",
        "                    \"source\": file_path,\n",
        "                    \"page_number\": sheet_name,\n",
        "                    \"title\": \" | \".join([str(cell) for cell in table_title]) if table_title else \"No Title\",\n",
        "                    \"table_data\": \"\\n\".join([\" | \".join([str(cell) for cell in row]) for row in current_table]),\n",
        "                    \"content\": \" | \".join([str(cell) for cell in table_title]) + \" | \" + \"\\n\".join([\" | \".join([str(cell) for cell in row]) for row in current_table])\n",
        "                }\n",
        "                tables.append(formatted_table)\n",
        "                table_title = None\n",
        "                current_table = []\n",
        "    if current_table:\n",
        "        formatted_table = {\n",
        "            \"type\": \"table\",\n",
        "            \"source\": file_path,\n",
        "            \"page_number\": sheet_name,\n",
        "            \"title\": \" | \".join([str(cell) for cell in table_title]) if table_title else \"No Title\",\n",
        "            \"table_data\": \"\\n\".join([\" | \".join([str(cell) for cell in row]) for row in current_table]),\n",
        "            \"content\": \" | \".join([str(cell) for cell in table_title]) + \" | \" + \"\\n\".join([\" | \".join([str(cell) for cell in row]) for row in current_table])\n",
        "        }\n",
        "        tables.append(formatted_table)\n",
        "    return tables\n",
        "\n",
        "def process_and_save_chunks(file_path):\n",
        "    \"\"\"\n",
        "    Processes a document to extract and chunk its content, saving the chunks in CSV files.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): The path to the document (PDF, DOCX, CSV, Excel, etc.).\n",
        "    \"\"\"\n",
        "    extracted_text = extract_text_from_file(file_path)\n",
        "    print(f\"Extracted text from {file_path}:\")\n",
        "\n",
        "    all_chunks = []\n",
        "    global_chunk_counter = 1\n",
        "    total_tokens = 0\n",
        "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    start_time = time.time()\n",
        "\n",
        "    for item in extracted_text:\n",
        "        if item[\"type\"] == \"table\":\n",
        "            chunk = {\n",
        "                \"content\": item[\"content\"],\n",
        "                \"page_number\": item[\"page_number\"],\n",
        "                \"chunk_number\": global_chunk_counter,\n",
        "                \"source\": item.get('source', 'Unknown'),\n",
        "                \"is_table\": True\n",
        "            }\n",
        "            all_chunks.append(chunk)\n",
        "            total_tokens += len(encoder.encode(chunk[\"content\"]))\n",
        "            global_chunk_counter += 1\n",
        "        else:\n",
        "            chunks, global_chunk_counter = tokenize_and_chunk_semantic(item, global_chunk_counter)\n",
        "            for chunk in chunks:\n",
        "                chunk[\"is_table\"] = False\n",
        "                total_tokens += len(encoder.encode(chunk[\"content\"]))\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    tps = total_tokens / total_time if total_time > 0 else 0\n",
        "\n",
        "    print(f\"\\n--- Performance Stats ---\")\n",
        "    print(f\"Total tokens processed: {total_tokens}\")\n",
        "    print(f\"Total time: {total_time:.2f} seconds\")\n",
        "    print(f\"Tokens per second (TPS): {tps:.2f}\")\n",
        "\n",
        "    # Save the full smart (table-aware) version\n",
        "    smart_csv = file_name + \"_smart_table_aware_chunking_chunked_text_details.csv\"\n",
        "    with open(smart_csv, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"source\", \"page_number\", \"chunk_number\", \"content\", \"is_table\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(all_chunks)\n",
        "    print(f\"Smart table-aware chunks saved to: {smart_csv}\")\n",
        "\n",
        "    # Save a simplified version (without `is_table`)\n",
        "    simple_csv = file_name + \"_chunked_text_details.csv\"\n",
        "    with open(simple_csv, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"source\", \"page_number\", \"chunk_number\", \"content\"])\n",
        "        writer.writeheader()\n",
        "        for row in all_chunks:\n",
        "            writer.writerow({k: v for k, v in row.items() if k != \"is_table\"})\n",
        "    print(f\"Clean/simple chunks saved to: {simple_csv}\")\n",
        "\n",
        "# Example usage\n",
        "# file_path = \"/your/local/path/to/document.xlsx\"  # Change to your file path\n",
        "file_path = \"/content/drive/MyDrive/osos_technical_testing_folder/Dr.X Files/Ocean_ecogeochemistry_A_review.pdf\"\n",
        "process_and_save_chunks(file_path)\n"
      ],
      "metadata": {
        "id": "O2zxlgWg83H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# improve the translation accuracy and fluency of the tool, I will integrate the following enhancements into the existing code:\n",
        "\n",
        "# Context-Aware Chunking: We'll adjust the chunking strategy to better preserve context.\n",
        "\n",
        "# Dynamic Tokenization: We'll adjust chunk sizes based on token limits to preserve context better."
      ],
      "metadata": {
        "id": "hpw_JhBr_wbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Force CPU usage for all models\n",
        "# ----------------------------\n",
        "device = -1  # CPU\n",
        "print(\"[*] Device set to: CPU (forced to avoid GPU memory errors)\")\n",
        "\n",
        "# ----------------------------\n",
        "# Load Translation Pipelines\n",
        "# ----------------------------\n",
        "print(\"[*] Loading translation models...\")\n",
        "\n",
        "translator_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\", device=device)\n",
        "\"\"\" Translation pipeline for multilingual input to English \"\"\"\n",
        "\n",
        "translator_en_to_ar = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ar\", device=device)\n",
        "\"\"\" Translation pipeline from English to Arabic \"\"\"\n",
        "\n",
        "# ----------------------------\n",
        "# Load Grammar Correction Model (English only)\n",
        "# ----------------------------\n",
        "fluency_model_id = \"vennify/t5-base-grammar-correction\"\n",
        "fluency_tokenizer = AutoTokenizer.from_pretrained(fluency_model_id)\n",
        "fluency_model = AutoModelForSeq2SeqLM.from_pretrained(fluency_model_id)\n",
        "\"\"\" Loads grammar correction model for polishing English text \"\"\"\n",
        "\n",
        "# ----------------------------\n",
        "# Text Extraction Functions\n",
        "# ----------------------------\n",
        "def extract_text(file_path):\n",
        "    \"\"\"\n",
        "    Detects the file format and extracts text accordingly.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input file (.pdf, .docx, .txt)\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text content.\n",
        "    \"\"\"\n",
        "    ext = file_path.lower().split('.')[-1]\n",
        "    if ext == \"pdf\":\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif ext == \"docx\":\n",
        "        return extract_from_docx(file_path)\n",
        "    elif ext == \"txt\":\n",
        "        return extract_from_txt(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "def extract_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text from PDF file using pdfplumber.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to PDF.\n",
        "\n",
        "    Returns:\n",
        "        str: Combined text from all pages.\n",
        "    \"\"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        return \"\\n\\n\".join([page.extract_text() or \"\" for page in pdf.pages])\n",
        "\n",
        "def extract_from_docx(docx_path):\n",
        "    \"\"\"\n",
        "    Extract text from DOCX file.\n",
        "\n",
        "    Args:\n",
        "        docx_path (str): Path to DOCX file.\n",
        "\n",
        "    Returns:\n",
        "        str: Combined paragraph text.\n",
        "    \"\"\"\n",
        "    doc = Document(docx_path)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "def extract_from_txt(txt_path):\n",
        "    \"\"\"\n",
        "    Extract text from plain text file.\n",
        "\n",
        "    Args:\n",
        "        txt_path (str): Path to TXT file.\n",
        "\n",
        "    Returns:\n",
        "        str: File content.\n",
        "    \"\"\"\n",
        "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "# ----------------------------\n",
        "# Text Chunking Helper\n",
        "# ----------------------------\n",
        "def split_text(text, max_len=512):\n",
        "    \"\"\"\n",
        "    Splits long text into smaller chunks based on sentence boundaries.\n",
        "\n",
        "    Args:\n",
        "        text (str): Full raw text.\n",
        "        max_len (int): Maximum characters allowed per chunk.\n",
        "\n",
        "    Returns:\n",
        "        list: List of text chunks.\n",
        "    \"\"\"\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    chunks, current = [], ''\n",
        "    for sent in sentences:\n",
        "        if len(current) + len(sent) < max_len:\n",
        "            current += sent + ' '\n",
        "        else:\n",
        "            chunks.append(current.strip())\n",
        "            current = sent + ' '\n",
        "    if current:\n",
        "        chunks.append(current.strip())\n",
        "    return chunks\n",
        "\n",
        "# ----------------------------\n",
        "# Translation Function\n",
        "# ----------------------------\n",
        "def translate_text(text, target_lang=\"en\"):\n",
        "    \"\"\"\n",
        "    Translates given text into the target language ('en' or 'ar').\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text.\n",
        "        target_lang (str): Target language code ('en' or 'ar').\n",
        "\n",
        "    Returns:\n",
        "        str: Translated text.\n",
        "    \"\"\"\n",
        "    print(f\"[*] Translating to {target_lang.upper()}...\")\n",
        "    chunks = split_text(text)\n",
        "    translated_chunks = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    total_tokens = 0\n",
        "\n",
        "    for chunk in chunks:\n",
        "        model = translator_to_en if target_lang == \"en\" else translator_en_to_ar\n",
        "        tokenized_input = model.tokenizer(chunk, return_tensors=\"pt\")\n",
        "        num_tokens = len(tokenized_input[\"input_ids\"][0])\n",
        "        total_tokens += num_tokens\n",
        "        translated = model(chunk, max_length=512)\n",
        "        translated_chunks.append(translated[0][\"translation_text\"])\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"[*] Processed {total_tokens} tokens in {elapsed_time:.2f} seconds.\")\n",
        "    print(f\"[*] Tokens per second: {total_tokens / elapsed_time:.2f}\")\n",
        "\n",
        "    return \"\\n\\n\".join(translated_chunks)\n",
        "\n",
        "# ----------------------------\n",
        "# Grammar Correction\n",
        "# ----------------------------\n",
        "def improve_fluency(text):\n",
        "    \"\"\"\n",
        "    Improves fluency of English text using a grammar correction model.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input English text.\n",
        "\n",
        "    Returns:\n",
        "        str: Polished, grammatically correct text.\n",
        "    \"\"\"\n",
        "    print(\"[*] Polishing English translation (fluency)...\")\n",
        "    input_text = \"grammar: \" + text.strip().replace(\"\\n\", \" \")\n",
        "    inputs = fluency_tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = fluency_model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n",
        "    return fluency_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Save Output\n",
        "# ----------------------------\n",
        "def save_as_docx(text, output_path):\n",
        "    \"\"\"\n",
        "    Saves the final text as a DOCX file.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be saved.\n",
        "        output_path (str): File path to save the DOCX.\n",
        "    \"\"\"\n",
        "    doc = Document()\n",
        "    for para in text.split(\"\\n\\n\"):\n",
        "        doc.add_paragraph(para.strip())\n",
        "    doc.save(output_path)\n",
        "\n",
        "# ----------------------------\n",
        "# Main Processor\n",
        "# ----------------------------\n",
        "def process_document(file_path, target_lang=\"en\"):\n",
        "    \"\"\"\n",
        "    Main function to process a document: extract, translate, polish (if needed), and save.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Input file path (.pdf, .docx, .txt)\n",
        "        target_lang (str): Target language code ('en' or 'ar')\n",
        "    \"\"\"\n",
        "    print(f\"\\n[+] Processing file: {file_path}\")\n",
        "    text = extract_text(file_path)\n",
        "    detected_lang = detect(text)\n",
        "    print(f\"[*] Detected source language: {detected_lang}\")\n",
        "\n",
        "    if target_lang == \"ar\" and detected_lang != \"en\":\n",
        "        print(\"[*] Source is not English. Translating to English first...\")\n",
        "        text_in_english = translate_text(text, target_lang=\"en\")\n",
        "        text_in_english = improve_fluency(text_in_english)\n",
        "        print(\"[*] Translating English to Arabic...\")\n",
        "        final_translation = translate_text(text_in_english, target_lang=\"ar\")\n",
        "    else:\n",
        "        final_translation = translate_text(text, target_lang=target_lang)\n",
        "        if target_lang == \"en\":\n",
        "            final_translation = improve_fluency(final_translation)\n",
        "\n",
        "    suffix = \"_translated_to_\" + target_lang\n",
        "    output_path = os.path.splitext(file_path)[0] + suffix + \".docx\"\n",
        "    save_as_docx(final_translation, output_path)\n",
        "    print(f\"[✓] Saved translated file to: {output_path}\\n\")\n",
        "\n",
        "# ----------------------------\n",
        "# Example Usage\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/drive/MyDrive/osos_technical/csv_ouput_osos/other_language_files/osos_french.pdf\"  # Replace with your actual path\n",
        "    target_language = \"ar\"  # Change to 'en' for English output\n",
        "    process_document(file_path, target_lang=target_language)\n"
      ],
      "metadata": {
        "id": "88gL5Gc7DXls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# improve evaluation metrics for algorithms"
      ],
      "metadata": {
        "id": "WucOl3CED46k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics for translation task"
      ],
      "metadata": {
        "id": "hgRoZJw7WJRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
        "\n",
        "\n",
        "def calculate_translation_preservation_score(original_text, back_translated_text):\n",
        "    \"\"\"\n",
        "    Calculates the semantic similarity between the original and back-translated texts\n",
        "    to evaluate translation accuracy.\n",
        "\n",
        "    This metric helps assess how much of the original meaning is preserved after translation\n",
        "    and back-translation using cosine similarity between sentence embeddings.\n",
        "\n",
        "    Args:\n",
        "        original_text (str): The original text before translation.\n",
        "        back_translated_text (str): The back-translated version of the translated text.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the translation preservation score (float between -1 and 1).\n",
        "    \"\"\"\n",
        "    print(\"[*] Calculating Translation Preservation Score...\")\n",
        "    orig_embed = embedding_model.encode([original_text], convert_to_tensor=True)\n",
        "    back_embed = embedding_model.encode([back_translated_text], convert_to_tensor=True)\n",
        "    score = cosine_similarity(orig_embed.cpu().numpy(), back_embed.cpu().numpy())[0][0]\n",
        "    print(f\"[✓] Translation Preservation Score: {score:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "original_text = \"Ceci est un document d’exemple contenant plusieurs phrases en espagnol. Le but de ce texte est de \\\n",
        "servir d’entrée pour les tests de traduction dans le système.  \\\n",
        "Les tests de traduction doivent être précis et efficaces. Assurez-vous que le contenu conserve sa \\\n",
        "structure et sa fluidité après la traduction. Ce document sera traduit en anglais ou en arabe selon ce qui \\\n",
        "est spécifié\"\n",
        "\n",
        "back_translated_text = \"This is an example document containing several sentences in Spanish. The purpose of this text is to use input for translation tests in the system. Translation tests must be accurate and effective. Make sure that the content preserves its structure and fluidity after translation. This document will be translated into English or Arabic according to what is specified.\"\n",
        "calculate_translation_preservation_score(original_text, back_translated_text)\n"
      ],
      "metadata": {
        "id": "7AfklD5MWOKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chunking and embedding evaluation with semantic overlap evalution metrics."
      ],
      "metadata": {
        "id": "LeXDvUQDYRLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Sentence-BERT model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def load_embeddings_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Loads embeddings from a CSV file where each embedding is stored as a stringified list in the 'embedding' column.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): Path to the CSV file containing the embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: A 2D numpy array where each row is a vector embedding.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    embeddings = df['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=',')).tolist()\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def calculate_semantic_overlap(embeddings):\n",
        "    \"\"\"\n",
        "    Calculates the average cosine similarity between adjacent embedding vectors to estimate semantic coherence.\n",
        "\n",
        "    Args:\n",
        "    - embeddings (np.ndarray): A 2D array of vector embeddings, one per chunk.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average cosine similarity between adjacent chunks.\n",
        "    \"\"\"\n",
        "    similarities = []\n",
        "    for i in range(len(embeddings) - 1):\n",
        "        similarity = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]\n",
        "        similarities.append(similarity)\n",
        "\n",
        "    avg_similarity = np.mean(similarities)\n",
        "    return avg_similarity\n",
        "\n",
        "# === Example Usage ===\n",
        "\n",
        "# Path to the CSV file containing chunk embeddings\n",
        "embedding_csv_file = '/content/drive/MyDrive/osos_technical/csv_ouput_osos/embedding_csv/Dataset summaries and citations_chunked_text_details.csv_embedding_results.csv'  # Replace with your actual path\n",
        "\n",
        "# Load the embeddings from the CSV\n",
        "embeddings = load_embeddings_from_csv(embedding_csv_file)\n",
        "\n",
        "# Calculate and print the semantic overlap score\n",
        "semantic_overlap_score = calculate_semantic_overlap(embeddings)\n",
        "print(f\"Average Semantic Overlap Score between Chunks: {semantic_overlap_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "okB8hAPbd_uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary evalutation metrics - factual_consistency_score"
      ],
      "metadata": {
        "id": "Hw1lzZNdmlOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Load a QA pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Original text and summary\n",
        "original_text = \"\"\"\n",
        "This is a long publication about advancements in AI. Artificial intelligence (AI) is the simulation of human intelligence in machines.\n",
        "The goal is to create systems capable of performing tasks that typically require human intelligence. These tasks include speech recognition,\n",
        "decision-making, and visual perception. In recent years, AI has seen significant growth in applications ranging from self-driving cars\n",
        "to healthcare diagnostics. With advancements in deep learning and neural networks, AI is expected to revolutionize industries worldwide.\n",
        "However, challenges like ethical considerations and data privacy remain.\n",
        "\"\"\"\n",
        "\n",
        "summary = \"\"\"\n",
        "Artificial intelligence is the simulation of human intelligence in machines. In recent years, AI has seen significant growth in applications\n",
        "ranging from self-driving cars to healthcare diagnostics. With advancements in deep learning and neural networks, AI is expected to\n",
        "revolutionize industries worldwide.\n",
        "\"\"\"\n",
        "\n",
        "# Generate key questions from the summary to evaluate consistency\n",
        "questions = [\n",
        "    \"What is artificial intelligence?\",\n",
        "    \"What are some applications of AI?\",\n",
        "    \"What recent advancements are helping AI?\",\n",
        "    \"What impact is AI expected to have on industries?\",\n",
        "]\n",
        "\n",
        "def get_answers(text, questions):\n",
        "    \"\"\"\n",
        "    Extract answers from a given text for a list of questions using a QA pipeline.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to extract answers from.\n",
        "        questions (list): A list of questions (str) to be asked about the text.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of answer strings returned by the QA pipeline.\n",
        "    \"\"\"\n",
        "    return [qa_pipeline(question=q, context=text)[\"answer\"] for q in questions]\n",
        "\n",
        "def factual_consistency_score(orig_ans, summ_ans):\n",
        "    \"\"\"\n",
        "    Compute a simple factual consistency score between original and summary answers.\n",
        "\n",
        "    Parameters:\n",
        "        orig_ans (list): Answers extracted from the original text.\n",
        "        summ_ans (list): Answers extracted from the summary.\n",
        "\n",
        "    Returns:\n",
        "        float: Factual consistency score (0 to 1), based on matching answers.\n",
        "               A score of 1.0 means perfect factual alignment.\n",
        "    \"\"\"\n",
        "    matches = [\n",
        "        1 if a1.lower() in a2.lower() or a2.lower() in a1.lower() else 0\n",
        "        for a1, a2 in zip(orig_ans, summ_ans)\n",
        "    ]\n",
        "    return np.mean(matches)\n",
        "\n",
        "# Get answers from both original and summary\n",
        "original_answers = get_answers(original_text, questions)\n",
        "summary_answers = get_answers(summary, questions)\n",
        "\n",
        "# Calculate and print score\n",
        "score = factual_consistency_score(original_answers, summary_answers)\n",
        "print(f\"\\nFactual Consistency Score: {score:.2f}\")\n"
      ],
      "metadata": {
        "id": "TytbnRfDg1JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG System evaluation"
      ],
      "metadata": {
        "id": "jUApwxdBOz0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def calculate_answer_grounding_score(answer: str, retrieved_chunks: list) -> float:\n",
        "    \"\"\"\n",
        "    Calculates how well a generated answer is grounded in the provided retrieved context using cosine similarity.\n",
        "\n",
        "    This function encodes the given answer and a list of retrieved text chunks into vector embeddings using\n",
        "    a SentenceTransformer model. It then computes the cosine similarity between the answer and each chunk\n",
        "    to determine the degree of semantic alignment. The highest similarity score among all chunks is returned\n",
        "    as the grounding score.\n",
        "\n",
        "    Args:\n",
        "        answer (str): The generated answer whose grounding needs to be evaluated.\n",
        "        retrieved_chunks (list of str): A list of text chunks retrieved from the context or knowledge base.\n",
        "\n",
        "    Returns:\n",
        "        float: The maximum cosine similarity score between the answer and any of the retrieved chunks.\n",
        "               A score closer to 1 indicates stronger grounding.\n",
        "    \"\"\"\n",
        "    print(\"[*] Calculating Answer Grounding Score...\")\n",
        "    # Encoding the answer and the retrieved chunks to embeddings\n",
        "    answer_embedding = embedding_model.encode(answer, convert_to_tensor=True)\n",
        "    chunk_embeddings = embedding_model.encode(retrieved_chunks, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarities between the answer and each retrieved chunk\n",
        "    similarities = util.pytorch_cos_sim(answer_embedding, chunk_embeddings)[0]\n",
        "\n",
        "    # Get the maximum similarity score, which indicates the best-grounded chunk\n",
        "    max_similarity_score = similarities.max().item()\n",
        "\n",
        "    print(f\"[✓] Answer Grounding Score: {max_similarity_score:.4f}\")\n",
        "    return max_similarity_score\n",
        "\n",
        "# Example Answer\n",
        "answer = \"The basis for seasonality in the analysis of soil carbon dynamics in residential lawns is based on findings by Trammell et al. (2019). They showed that C3 species dominated lawns in Los Angeles, CA. The approach used here is different because it uses a different dataset and a different method to estimate SOC sequestration.\"\n",
        "\n",
        "# Example Retrieved Chunks (Footnotes, Text, etc.)\n",
        "retrieved_chunks = [\n",
        "    \"1. The total sequestration reported here differs from that reported by Br | au | n and Bremer (2019), because they reported mean rather than total SOC sequestration rate across depths.\",\n",
        "    \"2. Campbell et al., (2014) reported 0-5 cm depth with a linear fit, and did not report the non-linear relationships for deeper intervals. We fit 3rd order polynomials to all intervals.\",\n",
        "    \"3. Authors used linear regression in Huyler et al., 2014 and localized polynomial fitting in Huyler et al., 2017. We applied the most parsimonious polynomial regressions to each depth interval, which were a linear, 2nd order, and third order polynomial to the top, mid, and bottom interval, respectively.\",\n",
        "    \"4. Raciti et al., (2011) applied a linear regression to the whole profile, but the 10-30, 30-70, and 70-100 cm intervals had non-linear trends. We fit these intervals with 3rd order polynomials, and fit a linear regression to the whole profile.\",\n",
        "    \"5. Seasonality for these cities is based on findings by Trammell et al. (2019), https://doi.org/10.1002/eap.1884. Contrary to expectation based on climate, they showed C3 species dominated lawns in Los Angeles, CA.\",\n",
        "    \"Raciti, S. M., Groffman, P. M., Jenkins, J. C., Pouyat, R. V., Fahey, T. J., Pickett, S. T. A., & Cadenasso, M. L. (2011). Accumulation of Carbon and Nitrogen in Residential Soils with Different Land-Use Histories. Ecosystems, 14 (2), 287–297. https://doi.org/10.1007/s10021-010-9409-3\"\n",
        "]\n",
        "\n",
        "\n",
        "# Calculate the Answer Grounding Score\n",
        "answer_grounding_score = calculate_answer_grounding_score(answer, retrieved_chunks)\n",
        "print(f\"Answer Grounding Score: {answer_grounding_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "f4mw3IBCuMW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement create algorithms for tables and charts text extraction."
      ],
      "metadata": {
        "id": "A6Ws28PnuySH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tables_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts tables from a PDF file using pdfplumber.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: A list of dictionaries where each dictionary represents a row\n",
        "                      from the extracted tables, with column headers as keys.\n",
        "    \"\"\"\n",
        "    tables = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            table = page.extract_table()\n",
        "            if table:\n",
        "                headers = [h.strip() if h else f\"Column{i}\" for i, h in enumerate(table[0])]\n",
        "                for row in table[1:]:\n",
        "                    if row:\n",
        "                        row_dict = {headers[i]: cell for i, cell in enumerate(row)}\n",
        "                        tables.append(row_dict)\n",
        "    return tables\n",
        "\n",
        "\n",
        "def extract_tables_from_docx(docx_path):\n",
        "    \"\"\"\n",
        "    Extracts tables from a DOCX file using python-docx.\n",
        "\n",
        "    Args:\n",
        "        docx_path (str): Path to the DOCX file.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: A list of dictionaries where each dictionary represents a row\n",
        "                      from the extracted tables, with column headers as keys.\n",
        "    \"\"\"\n",
        "    doc = docx.Document(docx_path)\n",
        "    table_data = []\n",
        "    for table in doc.tables:\n",
        "        keys = [cell.text.strip() for cell in table.rows[0].cells]\n",
        "        for row in table.rows[1:]:\n",
        "            values = [cell.text.strip() for cell in row.cells]\n",
        "            if len(values) == len(keys):\n",
        "                table_data.append(dict(zip(keys, values)))\n",
        "    return table_data\n",
        "\n",
        "\n",
        "def extract_text_from_csv(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV file and returns its contents as a list of dictionaries.\n",
        "\n",
        "    Tries multiple common encodings to handle potential encoding issues.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: A list of dictionaries representing rows in the CSV file.\n",
        "\n",
        "    Raises:\n",
        "        UnicodeDecodeError: If the file cannot be decoded using common encodings.\n",
        "    \"\"\"\n",
        "    for encoding in ['utf-8', 'ISO-8859-1', 'cp1252']:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            return df.to_dict(orient='records')\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "    raise UnicodeDecodeError(\"Unable to decode CSV with common encodings.\")\n",
        "\n",
        "\n",
        "def extract_text_from_excel(file_path):\n",
        "    \"\"\"\n",
        "    Reads an Excel file and returns its contents as a list of dictionaries.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the Excel file (.xls or .xlsx).\n",
        "\n",
        "    Returns:\n",
        "        list of dict: A list of dictionaries representing rows in the Excel file.\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(file_path)\n",
        "    return df.to_dict(orient='records')\n",
        "\n",
        "\n",
        "def extract_structured_text(file_path):\n",
        "    \"\"\"\n",
        "    Extracts structured table/chart data from various file formats including:\n",
        "    PDF, DOCX, CSV, XLS, XLSX.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: Extracted structured data from the file.\n",
        "\n",
        "    Raises:\n",
        "        NotImplementedError: If attempting to process unsupported .txt files.\n",
        "        ValueError: If the file extension is not supported.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(file_path)[-1].lower()\n",
        "\n",
        "    if ext == '.pdf':\n",
        "        return extract_tables_from_pdf(file_path)\n",
        "\n",
        "    elif ext == '.docx':\n",
        "        return extract_tables_from_docx(file_path)\n",
        "\n",
        "    elif ext == '.txt':\n",
        "        raise NotImplementedError(\"Text files are not supported for structured chart/table extraction.\")\n",
        "\n",
        "    elif ext == '.csv':\n",
        "        return extract_text_from_csv(file_path)\n",
        "\n",
        "    elif ext in ['.xls', '.xlsx']:\n",
        "        return extract_text_from_excel(file_path)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "# give file path here\n",
        "file_path = \"/content/drive/MyDrive/osos_technical_testing_folder/Dr.X Files/party budget1.xlsx\"\n",
        "structured_output = extract_structured_text(file_path)\n",
        "\n",
        "print(\"Extracted Structured Table/Chart Data:\")\n",
        "for row in structured_output:\n",
        "  print(row)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLdCzDtswAmJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}